{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366360e7",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer With Multiple Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897a5f23",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'boxplot_plotting' from 'src.plot_boxplot' (/Users/clichy/Desktop/FinalProject/DSCI-310-Group-5/doc/jbook/../../src/plot_boxplot.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmean_cross_val_scores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_cross_val_scores\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_boxplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m boxplot_plotting\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_hist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_hist_overlay\n\u001b[1;32m     10\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'boxplot_plotting' from 'src.plot_boxplot' (/Users/clichy/Desktop/FinalProject/DSCI-310-Group-5/doc/jbook/../../src/plot_boxplot.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append( '../..' )\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from src.mean_cross_val_scores import mean_cross_val_scores\n",
    "from src.plot_boxplot import boxplot_plotting\n",
    "from src.plot_hist import plot_hist_overlay\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split\n",
    ")\n",
    "from myst_nb import glue\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9a5bc",
   "metadata": {},
   "source": [
    "## I. Summary\n",
    "\n",
    "In this project, we perform data analysis to look for the most efficient model to predict whether it is benign and malignant tumour based on different measurements/traits observed from the data. We use various machine learning algorithms that classify whether a breast tumour is malignant or benign accordingly, and eventually decide on the best algorithm for that task based on recall score. The dataset is obtained from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28original%29\n",
    "\n",
    "## II. Introduction\n",
    "\n",
    "### Background\n",
    "\n",
    "Breast cancer is the development of cancerous tissue of the breast. A malignant (cancerous) tumour can destroy other healthy tissue surrounding it and the disease could potentially metastasize, causing numerous other health complications. However, a tumour might or might not be cancerous. A benign (non-cancerous) tumour will not cause as much harm to the patient compared to a malignant one. With that in mind, an early diagnosis of a **malignant tumour** would give both patients and health care providers valuable time to quickly devise various treatment plans to stop the progression of the disease. This is important especially for a silent killer like cancer, in which the earlier a correct diagnosis has been made, the drastically better the prognosis {cite}`MLbreastCancer, BreastCancer, DeepLearnCancer`.\n",
    "This raises the question:\n",
    "\n",
    "**Can we predict whether a tumor is malignant or benign based  on the different features observed from the tumour?**\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset we use: Breast Cancer Wisconsin (Original) Data Set, created by Dr. WIlliam H. Wolberg (physician), contains the following features with 699 observations {cite}`Database`:\n",
    "\n",
    "1. Sample code number: id number\n",
    "2. Clump Thickness: 1 - 10\n",
    "3. Uniformity of Cell Size: 1 - 10\n",
    "4. Uniformity of Cell Shape: 1 - 10\n",
    "5. Marginal Adhesion: 1 - 10\n",
    "6. Single Epithelial Cell Size: 1 - 10\n",
    "7. Bare Nuclei: 1 - 10\n",
    "8. Bland Chromatin: 1 - 10\n",
    "9. Normal Nucleoli: 1 - 10\n",
    "10. Mitoses: 1 - 10\n",
    "\n",
    "With the relevant target class:\n",
    "\n",
    "11. Class: (2 for benign, 4 for malignant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320919e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## III. Method & Results\n",
    "\n",
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f7a1d",
   "metadata": {},
   "source": [
    "Since the dataset did not come with column headers, we will first manually add them and load data. The headers will correspond to the order of their respective description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd1f53",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "col_names = [\"id\", \"clump\", \"unif_size\", \"unif_shape\", \"adhesion\", \"epi_size\",\n",
    "             \"nuclei\", \"chromatin\", \"nucleoli\", \"mitoses\", \"class\"]\n",
    "\n",
    "dataset = pd.read_csv(\"../../data/raw/breast_cancer.txt\", names=col_names, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286df32",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "head = dataset.head()\n",
    "glue(\"header\", head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e77267",
   "metadata": {},
   "source": [
    "```{glue:} header\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb120b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1723f7",
   "metadata": {},
   "source": [
    "We see that the dataset uses \"?\" for missing data so we eliminate rows that contain \"?\". As shown above, all of the variables are numeric except for variables nuclei, we decide to transform it into type int for ease of data analysis later on. Finally, \"id\" feature does not appear to be useful for the prediction task; hence, it is dropped before carrying on to further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc96a09",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = dataset[(dataset != '?').all(axis=1)]\n",
    "dataset['nuclei'] = dataset['nuclei'].astype(int)\n",
    "dataset = dataset.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69fd92c",
   "metadata": {},
   "source": [
    "We also decide to replace benign class from 2 to 0 and malignant class from 4 to 1 since if we keep values of 2 and 4, it would be hard for predictive models to calculate accuracy, precision, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac9aa4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "dataset['class'] = dataset['class'].replace([2],0)\n",
    "dataset['class'] = dataset['class'].replace([4],1) \n",
    "values = dataset['class'].value_counts(normalize = True)\n",
    "glue(\"vals\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc629542",
   "metadata": {},
   "source": [
    "```{glue:} vals\n",
    "\n",
    "```\n",
    "\n",
    "There is an imbalance in the dataset between benign and malignant. We want to further investigate the malignant examples (class = 4); hence the classes should have same importance so recall would be the most appropriate and main metric for this project. This is due to the fact that in a medical diagnosis setting, false negative result for a disease like cancer will cause a lot more harm than a false positive result.\n",
    "\n",
    "Then we split the data into training and testing sets (X_train, X_test, y_train, y_test) and extract names of numeric features for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc540456",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dataset, test_size=0.3, random_state=123)\n",
    "X_train = train_df.drop(columns=[\"class\"])\n",
    "X_test = test_df.drop(columns=[\"class\"])\n",
    "\n",
    "y_train = train_df[\"class\"]\n",
    "y_test = test_df[\"class\"]\n",
    "numeric_looking_columns = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08adf75",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "benign_cases = train_df[train_df[\"class\"] == 0]\n",
    "malignant_cases = train_df[train_df[\"class\"] == 1]\n",
    "print(len(numeric_looking_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a96eca",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plot_hist_overlay(df0=benign_cases, df1=malignant_cases,\n",
    "                 columns=numeric_looking_columns, labels=[\"0 - benign\", \"1 - malignant\"],\n",
    "                 fig_no=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9372d6f",
   "metadata": {},
   "source": [
    "```{figure} ../../results/figures/hist_plot.png\n",
    "---\n",
    "height: 700px\n",
    "name: Histograms\n",
    "---\n",
    "Histograms of Class Targets with respecto to each Explanatory Variable \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df31dcb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig = boxplot_plotting(3,3,20,25,numeric_looking_columns,train_df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d42b30",
   "metadata": {},
   "source": [
    "```{figure} ../../results/figures/boxplot_plot.png\n",
    "---\n",
    "height: 700px\n",
    "name: Boxplots\n",
    "---\n",
    "Boxplots of Class Targets with respecto to each Explanatory Variable \n",
    "```\n",
    "\n",
    "Judging from the {ref}`histograms <Histograms>`, it is indicated that benign tumors tend to have smaller values for all the features listed in the dataset (clump, unif_size and so on) while malignant tumors have different tendencies for each features.\n",
    "\n",
    "From the {ref}`boxplots <Boxplots>` above, chromatin and clump feature display the clearest differentiation between benign and malignant tumor. Benign tumors seem to have clump thickness and bland chromatin on the lower ends. These characteristics are also displayed clearly in the histograms. Moreover, it might not be as obvious in {ref}`boxplot <Boxplots>`, but in  {ref}`histogram <Histograms>`, the majority of benign tumors is on the lower end of the nuclei's spectrum while the majority of malignant tumors are on the upper end. This suggests tumors with higher values of clump thickness, chromatin and nuclei have a higher chance of being malignant. \n",
    "\n",
    "As for unif_size, unif_shape, adhesion and nucleoli, the {ref}`histogram <Histograms>` show that regarding to benign tumors, these features tend to range in small values while malignant tumors' ranges are more evenly spread out. This proposes tumors with smaller values of said features are more likely to be benign. But the proposal is not as strong as the one discussed above as the distinction is not as prominent.\n",
    "\n",
    "Lastly, mitoses and epi_size feature indicate to have an overlap in values between benign and malignant tumors shown in  {ref}`histogram <Histograms>`. Hence, they might not be as useful in prediction task as other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2a6c4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "ct = make_column_transformer(\n",
    "    (numeric_transformer, numeric_looking_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7254eb",
   "metadata": {},
   "source": [
    "### Data analysis \n",
    "Since all features are numeric, we decide to scale our data to ensure that there is no bias presents when predicting results. \n",
    "\n",
    "Even though the main score we will be comparing when choosing the models is recall, we still want to look into accuracy and decision. Because between a model performs well on recall but have very low accuracy and precision and a model performs just a bit worse on recall but have excellent accuracy and precision, the latter model will still have an upper hand and be more preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91bb8",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc882d18",
   "metadata": {},
   "source": [
    "We decide to test 3 models: Decision Tree, kNN and Logistic Regression. Decision Tree, kNN, Logistic Regression are simple models with fast fit_time and moderate precision and accuracy and suitable for the classification/prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6334d7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "pipe_knn = make_pipeline(ct, KNeighborsClassifier(n_neighbors=5))\n",
    "pipe_dt = make_pipeline(ct, DecisionTreeClassifier(random_state=123))\n",
    "pipe_reg = make_pipeline(ct, LogisticRegression(max_iter=100000))\n",
    "\n",
    "classifiers = {\n",
    "    \"kNN\": pipe_knn,\n",
    "    \"Decision Tree\": pipe_dt,\n",
    "    \"Logistic Regression\" : pipe_reg\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for (name, model) in classifiers.items():\n",
    "    results[name] = mean_cross_val_scores(\n",
    "        model, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        return_train_score=True, \n",
    "        scoring = scoring\n",
    "    )\n",
    "\n",
    "pd.DataFrame(results).T\n",
    "\n",
    "table = pd.read_csv(\"../../results/tables/cross_val.csv\", index_col=0, sep=\",\")\n",
    "glue(\"df_table\",table)\n",
    "glue(\"knn-based-rec\", np.around(table.iloc[0][\"test_recall\"], 3))\n",
    "glue(\"knn-based-prec\", np.around(table.iloc[0][\"test_precision\"], 3))\n",
    "glue(\"knn-based-acc\", np.around(table.iloc[0][\"test_accuracy\"], 3))\n",
    "glue(\"logit-prec\", np.around(table.iloc[2][\"test_precision\"], 3))\n",
    "glue(\"logit-rec\", np.around(table.iloc[2][\"test_recall\"], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c389a",
   "metadata": {},
   "source": [
    "```{glue:} df_table\n",
    "```\n",
    "As shown above, kNN model is the best performing model with the highest `test_recall score`, {glue:text}`knn-based-rec`, and high `test_precision` and `test_accuracy`, {glue:text}`knn-based-prec` and {glue:text}`knn-based-acc` respectively. Even though Logistics Regression has higher precision score {glue:text}`logit-prec` than KNN's, its recall score is lower than those of kNN ({glue:text}`logit-rec` < {glue:text}`knn-based-rec` respectively). Overall, kNN model still performs better than Logistics Regression. Decision Tree performs the worst since it displays the lowest scores among 3 models. \n",
    "\n",
    "After choosing the most efficient model, kNN, we move onto tuning its hyperparameters to increase its performance. We decide to tune n_neighbors which determines the number of neighbors k and weights which determines weight function used in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657bcf3",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "search = GridSearchCV(pipe_knn,\n",
    "                      param_grid={'kneighborsclassifier__n_neighbors': range(1, 50),\n",
    "                                  'kneighborsclassifier__weights': ['uniform', 'distance']},\n",
    "                      cv=10, \n",
    "                      n_jobs=-1,  \n",
    "                      scoring=\"recall\", \n",
    "                      return_train_score=True)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021756b5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "tuned = pd.read_csv(\"../../results/tables/tuned_para.csv\", index_col=0, sep=\",\")\n",
    "glue(\"tuned_para\", tuned)\n",
    "glue(\"para1\", tuned.iloc[0][\"kneighborsclassifier__n_neighbors\"])\n",
    "glue(\"para2\", tuned.iloc[0][\"kneighborsclassifier__weights\"])\n",
    "glue(\"recall\", np.around(tuned.iloc[0][\"knn_best_score\"], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc0b26",
   "metadata": {},
   "source": [
    "```{glue:} tuned_para\n",
    "```\n",
    "\n",
    "After tuning hyperparameters, we successfully increase recall score from {glue:text}`knn-based-rec` to {glue:text}`recall` with hyperparameters `n_neighbors=` {glue:text}`para1` and `weights=` {glue:text}`para2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52eb13",
   "metadata": {},
   "source": [
    "### Model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84f172",
   "metadata": {},
   "source": [
    "We then apply tuned hyperparameters, `n_neighbors=` {glue:text}`para1` and `weights=` {glue:text}`para2`, to kNN model and test the model's generalization on test set. \n",
    "\n",
    "We also print a result plot and plot a confusion matrix for X_test and y_test to visualize the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d730d26",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pipe_knn_tuned = make_pipeline(ct,KNeighborsClassifier(n_neighbors=5, weights='uniform'))\n",
    "pipe_knn_tuned.fit(X_train, y_train)\n",
    "pipe_knn_tuned.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be7ebe",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    y_test, pipe_knn_tuned.predict(X_test), target_names=[\"benign\", \"malignant\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e31cc",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "predictions = pipe_knn_tuned.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=pipe_knn_tuned.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=pipe_knn_tuned.classes_)\n",
    "disp.plot()\n",
    "plt.title(\"Figure 3: Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba843d0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv(\"../../results/tables/classification_report.csv\", index_col=0, sep=\",\")\n",
    "glue(\"predict\", preds)\n",
    "glue(\"fin-avg-acc\", np.around(preds.iloc[2][\"recall\"], 3))\n",
    "glue(\"fin-avg-prec\", np.around(preds.iloc[4][\"precision\"], 3))\n",
    "glue(\"fin-avg-rec\", np.around(preds.iloc[4][\"recall\"], 3))\n",
    "glue(\"fin-avg-f1\", np.around(preds.iloc[4][\"f1-score\"], 3))\n",
    "glue(\"fin-mal-rec\", np.around(preds.iloc[1][\"recall\"], 3))\n",
    "glue(\"fin-be-rec\", np.around(preds.iloc[0][\"recall\"], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739e580",
   "metadata": {},
   "source": [
    "```{glue:} predict\n",
    "```\n",
    "\n",
    "```{figure} ../../results/figures/confusion_matrix.png\n",
    "---\n",
    "height: 500px\n",
    "name: Conf-Matrix\n",
    "---\n",
    "Confusion matrix for Algorithms Classification\n",
    "```\n",
    "\n",
    "After applying the model to the test set, we obtain the score of {glue:text}`fin-avg-acc` which indicates that this model generalizes well for prediction task. Moreover, from the plot above {ref}`(Figure 3) <Conf-Matrix>`, we can see that out of **205** observations in test set, we only falsely predict **1** tumor to be benign and **2** tumors to malignant. This aligns with our decision at the beginning of the project which is to optimize recall score of the model. \n",
    "\n",
    "With the recall score of {glue:text}`fin-mal-rec` for malignant tumors and {glue:text}`fin-be-rec` for benign tumors reported above, we have statistical evidence that the model generalizes well for the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2502d1d",
   "metadata": {},
   "source": [
    "## IV. Summary of results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190eee55",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_test, pipe_knn_tuned.predict(X_test), target_names=[\"benign\", \"malignant\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "glue(\"fin-avg-acc-per\", np.around(preds.iloc[2][\"recall\"], 3)*100)\n",
    "glue(\"fin-avg-prec-per\", np.around(preds.iloc[4][\"precision\"], 3)*100)\n",
    "glue(\"fin-avg-rec-per\", np.around(preds.iloc[4][\"recall\"], 3)*100)\n",
    "glue(\"fin-avg-f1-per\", np.around(preds.iloc[4][\"f1-score\"], 3)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc782bb7",
   "metadata": {},
   "source": [
    "```{glue:} predict\n",
    "```\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "The boxplots and histograms allow us to see how the displayed data is distributed across our dataset and how they differentiate benign and malignant tumors. We believe certain variables will give us significantly more information on how to correctly classify a tumor. Such features which can play a decisive factor in prediction task are: \n",
    "\n",
    "Chromatin (Figures {ref}` 1.7 <Histograms>` & {ref}` 2.7 <Boxplots>`): It is a variable which at first sight could serve us as a way to determine the class label for a given test example. The problem with this variable is the intersection between 2 & 4 in the x axis for figure {ref}` 1.7 <Histograms>`. This is problematic because the probability of falsely predicting an observation increases if the value of chromatin of said observation lies within that given interval.\n",
    "\n",
    "Nucleoli (Figures{ref}` 1.8 <Histograms>`  & {ref}` 2.8 <Boxplots>`): This feature plays an important role in classifying tumor types as shown in both plots. Observing {ref}` 1.8 <Histograms>` we notice that benign tumors tend to have a nucleoli value in between 0 and 2 (for discrete values), and we primarily see malignant tumors for values greater than 2. This is further supported in {ref}` 2.8 <Boxplots>`, where we see that only outliers from benign tumors are entangled with the malignant tumor's box and lower whisker.\n",
    "\n",
    "Other variables like nuclei or adhesion can also be useful, but if we were able to set them only for certain values of the \"x\" variable. The problem is that if we, for example, take nuclei as the decisive rule to classify, we would have almost a 50/50 chance of correctly classifying it if \"x\" was between 2 and 4 approximately. \n",
    "\n",
    "By comparing cross-val scores (recall scores) among kNN, Decision Tree, and Logistics Regression, we found that kNN performs the best with highest recall score. By using `GridSearchCV` to computationally optimize the hyperparameters for `KNeighborsClassifier`, it turns out our best model is `KNeighborsClassifier` with `n_neighbors=` {glue:text}`para1` and `weights=` {glue:text}`para2`. This model achieved an astounding {glue:text}`fin-avg-acc-per`**% overall accuracy**, {glue:text}`fin-avg-rec-per`**% recall**, {glue:text}`fin-avg-prec-per`**% precision and an F1-score of** {glue:text}`fin-avg-f1-per`**%** when deploying on test set. This means that the model generalized very well for this prediction problem, and with such a high recall we also achieved our goal of maximizing the True Positive and minimizing the False Negative instances, which would be of tremendous importance when it comes to computational-aided medical diagnostic.\n",
    "\n",
    "### Impacts of our Findings\n",
    "We hope to aid other researchers and medical professionals in breast cancer diagnostic process and treatment with our results. We also hope to inspire other researchers to build upon what we have to continually improve this machine learning model, or take on a different perspective and build other technologies that will hopefully be more powerful and geared towards the task than what we developed.\n",
    "\n",
    "### Future Questions and Research Area\n",
    "1. Are there any other features related to cancer tissue, either unique to breast cancer or universal to all types of cancer, that can further improve the accuracy metrics of machine learning models?\n",
    "1. Is there a better machine learning algorithm that could be seen as the most \"optimal\" to this prediction task?\n",
    "1. Develop a better systematic approach/procedure between medical professional and these technologies to diagnose breast cancer more efficiently.\n",
    "1. Research into developing better technologies, not limited to AI/ML, to aid with breast cancer (or any cancer) diagnostic.\n",
    "\n",
    "## V. References\n",
    "```{bibliography} ../../doc/jbook/references.bib\n",
    ":style: plain\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "source_map": [
   12,
   16,
   48,
   82,
   88,
   92,
   101,
   106,
   111,
   115,
   119,
   125,
   129,
   136,
   147,
   160,
   168,
   174,
   186,
   190,
   210,
   216,
   223,
   232,
   236,
   272,
   280,
   298,
   305,
   312,
   316,
   322,
   330,
   338,
   350,
   360,
   377,
   381,
   394
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}